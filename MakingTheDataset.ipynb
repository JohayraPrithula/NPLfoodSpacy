{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opening and Loading the food Review Dataset\n",
    "\n",
    "Review.csv contains the amazon food review dataset from kaggle. We will use this to built our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of total reviews: 568454\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = pd.read_csv(os.path.join(os.getcwd(),'Reviews.csv'))\n",
    "\n",
    "FoodReviewSummaryDataset = df[['Summary']].copy()\n",
    "\n",
    "FoodReviewSummaryDataset.dtypes\n",
    "\n",
    "print(\"The number of total reviews:\", end = ' ')\n",
    "print(len(FoodReviewSummaryDataset['Summary']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering the Summeries\n",
    "\n",
    "There are summaries with meaningless words which do not translate as a sentence. To be safe I will choose the summaries that are at least 15 characters long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of reviews after filtering: 1536\n"
     ]
    }
   ],
   "source": [
    "\n",
    "list = []\n",
    "\n",
    "for row in FoodReviewSummaryDataset['Summary']:\n",
    "\n",
    "    sentence = str(row)\n",
    "    words = sentence.split()\n",
    "\n",
    "    if len(words) > 15:\n",
    "        list.append(sentence)\n",
    "\n",
    "\n",
    "FoodDataset = pd.DataFrame(list)\n",
    "\n",
    "print(\"The number of reviews after filtering:\", end = ' ')\n",
    "print(len(list))\n",
    "\n",
    "\n",
    "FoodDataset.to_csv('FoodDataset.csv')\n",
    "np.savetxt(r'G:\\Codeworks\\NPLfoodSpacy\\FoodDataset.txt', FoodDataset.values, fmt='%s')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anotation of Data\n",
    "\n",
    "This text file is then annotated using this link: https://tecoholic.github.io/ner-annotator/\n",
    "\n",
    "Most of the review summaries did not have food names, which wasn't included in the dataset. The size of our json dataset after annotation is as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "324\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "file_name = ('annotations.json')\n",
    "\n",
    "with open(file_name, 'r', encoding='utf-8') as f:\n",
    "    my_data = json.load(f)\n",
    "\n",
    "print (len(my_data['annotations']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "274052c43c725464e6a2f02e90ebca89bf5bffe249a9afa33df2460c93f5d4ea"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
